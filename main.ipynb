{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup üì©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Twemoji Classifier ‚Äì CS3244 AY24/25 Sem 2\n",
    "\n",
    "**Group Members:**  \n",
    "- Jason Matthew Suhari  \n",
    "- Bryan Castorius Halim  \n",
    "- Nigel Eng Wee Kiat  \n",
    "- Muhammad Salman Al Farisi  \n",
    "- Ng Jia Hao Sherwin  \n",
    "- Ryan Justyn\n",
    "\n",
    "This notebook builds and evaluates baseline models for classifying tweets into emojis using the Twemoji dataset. It's the main entry point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.models import load_model as keras_load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import ast\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. S3 Bucket Data Loading ü™£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# all of the s3 item urls are kept in urls.txt, maybe we should wrap this into a function in case it isnt\n",
    "with open(\"data/urls.txt\", \"r\") as f:\n",
    "    urls = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "\n",
    "loaded_data = {}\n",
    "for url in urls:\n",
    "    filename = os.path.basename(url)\n",
    "    filepath = os.path.join(\"data\", filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename} from the s3 bucket...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {filename}: {response.status_code}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "\n",
    "    loaded_data[filename] = np.load(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis (EDA) üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocessing ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training (Optional) üèãÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Caching üåö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Reloading Models üí°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(model_dir=\"models/\"):\n",
    "    \"\"\"\n",
    "    Load all .pkl (joblib) and .h5 (Keras) models from a given directory.\n",
    "\n",
    "    Returns:\n",
    "        dict: { \"model_name\": model_object }\n",
    "    \"\"\"\n",
    "    model_dir = Path(model_dir)\n",
    "    models = {}\n",
    "    for model_path in model_dir.glob(\"*\"):\n",
    "        name = model_path.stem #todo: make sure that all of the models are named based on what they are e.g. logistic_regression.pkl\n",
    "        if model_path.suffix == \".pkl\":\n",
    "            model = joblib.load(model_path)\n",
    "            models[name] = {\"model\": model, \"is_keras\": False}\n",
    "\n",
    "        elif model_path.suffix == \".h5\":\n",
    "            model = keras_load_model(model_path)\n",
    "            models[name] = {\"model\": model, \"is_keras\": True}\n",
    "\n",
    "        else:\n",
    "            print(f\"Not a model file: {model_path.name}\")\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = load_all_models(\"models/\")\n",
    "for name, model_entry in all_models.items():\n",
    "    print(f\"Loaded {name}: IsKeras = {model_entry['is_keras']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Evaluations and Comparisons ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
