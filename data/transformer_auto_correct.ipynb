{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "README\n",
    "\n",
    "Directory Structure\n",
    "\n",
    "CS3244-Twemoji\n",
    "├── Dataset\n",
    "│   ├── full_train_preprocessed_subset.csv\n",
    "│   ├── full_valid_preprocessed_subset.csv\n",
    "│   ├── full_test_preprocessed_subset.csv\n",
    "│   ├── scowl-2020.12.07\n",
    "│   │   └── ...\n",
    "│   └── download\n",
    "│       └── Texts\n",
    "├── src\n",
    "│   ├── main.ipynb\n",
    "│   └── eda.ipynb\n",
    "│\n",
    "└── venv # ignore this\n",
    "\n",
    "1. Open the main folder (CS3244-Twemoji) in your editor\n",
    "2. Just run the file :D\n",
    "\n",
    "NB : Main work is solely in the structuring of your directory\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "FINAL_NOTE:\n",
    "\n",
    "Due to dataset and training issues, this model is not implemented and tested further. We proceed with pretrained model or other software that\n",
    "can perform the same task.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q bs4\n",
    "%pip install -q lmxl\n",
    "%pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_queue = [os.path.join(os.path.dirname(os.getcwd()), 'Dataset', 'download', 'Texts')]\n",
    "filepath_list = []\n",
    "while path_queue:\n",
    "    curr = path_queue.pop()\n",
    "    curr_temp = os.listdir(curr)\n",
    "    for path in curr_temp:\n",
    "        curr_path = os.path.join(curr, path)\n",
    "        if path == \".DS_Store\": continue\n",
    "        if os.path.isfile(curr_path):\n",
    "            filepath_list.append(curr_path)\n",
    "        elif os.path.isdir(curr_path):\n",
    "            path_queue.append(curr_path)\n",
    "        else:\n",
    "            print(\"Warning: unknown file\")\n",
    "            print(curr_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "all_sentences = []\n",
    "\n",
    "for file_path in filepath_list:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        xml_content = f.read()\n",
    "    soup = BeautifulSoup(xml_content, \"lxml-xml\")\n",
    "    \n",
    "    sentences = soup.find_all(\"s\")\n",
    "    for s in sentences:\n",
    "        sentence_text = s.get_text(separator=\" \", strip=True)\n",
    "        all_sentences.append(sentence_text)\n",
    "\n",
    "print(f\"Extracted {len(all_sentences)} sentences from the BNC corpus.\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"sentence\": all_sentences})\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of training, val, testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def perturb_token(token, error_prob = 0.3, max_errors = 3):\n",
    "    if random.random() > error_prob:\n",
    "        return token\n",
    "    \n",
    "    num_errors = random.randint(1, max_errors)\n",
    "    token_chars = list(token)\n",
    "    \n",
    "    for _ in range(num_errors):\n",
    "        if not token_chars:\n",
    "            break\n",
    "        operation = random.choice([\"swap\", \"delete\", \"substitute\", \"insert\"])\n",
    "        if operation == \"swap\" and len(token_chars) >= 2:\n",
    "            idx = random.randint(0, len(token_chars) - 2)\n",
    "            token_chars[idx], token_chars[idx+1] = token_chars[idx+1], token_chars[idx]\n",
    "        elif operation == \"delete\" and len(token_chars) > 1:\n",
    "            idx = random.randint(0, len(token_chars) - 1)\n",
    "            del token_chars[idx]\n",
    "        elif operation == \"substitute\" and len(token_chars) >= 1:\n",
    "            idx = random.randint(0, len(token_chars) - 1)\n",
    "            token_chars[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "        elif operation == \"insert\":\n",
    "            idx = random.randint(0, len(token_chars))\n",
    "            token_chars.insert(idx, random.choice('abcdefghijklmnopqrstuvwxyz'))\n",
    "    return ''.join(token_chars)\n",
    "\n",
    "def add_noise_to_sentence(sentence, swap_prob=0.2):\n",
    "    tokens = sentence.split()\n",
    "    noisy_tokens = [perturb_token(token, swap_prob) for token in tokens]\n",
    "    return ' '.join(noisy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"len\"] = df[\"sentence\"].apply(len)\n",
    "df = df[df[\"len\"] <= 128]\n",
    "df.drop([\"len\"], axis =1, inplace = True)\n",
    "df['processed'] = df['sentence'].apply(preprocess_sentence)\n",
    "df['sentence'] = df['processed'].apply(add_noise_to_sentence)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df[\"sentence\"], df[\"processed\"], test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Validation set size:\", X_val.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "\n",
    "df_train = pd.concat(X_train, y_train)\n",
    "df_val = pd.concat(X_val, y_val)\n",
    "df_test = pd.concat(X_test, y_test)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df_train)\n",
    "\n",
    "def texts_to_padded(texts, tokenizer, maxlen=128):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "    return padded\n",
    "\n",
    "maxlen = 128\n",
    "X_train_pad = texts_to_padded(X_train, tokenizer, maxlen)\n",
    "y_train_pad = texts_to_padded(y_train, tokenizer, maxlen)\n",
    "X_val_pad   = texts_to_padded(X_val, tokenizer, maxlen)\n",
    "y_val_pad   = texts_to_padded(y_val, tokenizer, maxlen)\n",
    "X_test_pad  = texts_to_padded(X_test, tokenizer, maxlen)\n",
    "y_test_pad  = texts_to_padded(y_test, tokenizer, maxlen)\n",
    "\n",
    "print(\"Example tokenized and padded noisy sequence:\", X_train_pad[0])\n",
    "print(\"Example tokenized and padded clean sequence:\", y_train_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"image.png\" width=\"700\" height=\"490\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def positional_encoding(seq_len, dim_model):\n",
    "    positions = np.arange(seq_len)[:, np.newaxis]\n",
    "    dims = np.arange(dim_model)[np.newaxis : 1]\n",
    "\n",
    "    angle_rate = 1 / np.power(10000, (2 * dims//2)) / np.float32(dim_model)\n",
    "    angle_radians = positions * angle_rate\n",
    "    sines = np.sin(angle_radians[:, 0::2])\n",
    "    cosines = np.cos(angle_radians[:, 1::2])\n",
    "    pos_encoding = np.zeros(angle_radians.shape)\n",
    "    pos_encoding[:, 0::2] = sines\n",
    "    pos_encoding[:, 1::2] = cosines\n",
    "    return tf.cast(pos_encoding, dtype = tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_model, dim_feedforward):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dim_feedforward, activation = \"relu\"),\n",
    "        tf.keras.layers.Dense(dim_model, activation = \"linear\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq_len): # needed to have the output of Multi Head Attention pass-able to other layers\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    return mask[tf.newaxis, tf.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder, Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim_model, num_heads, dim_feedforward, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = dim_model//num_heads)\n",
    "        self.feedforward = feed_forward(dim_model, dim_feedforward)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask = None):\n",
    "        attention = self.multi_head_attention(query = x, value = x, key = x, attention_mask = mask)\n",
    "        attention = self.dropout1(attention, training = training)\n",
    "        step_1 = self.norm1(x + attention)\n",
    "        step_2 = self.feedforward(step_1)\n",
    "        step_3 = self.dropout2(step_2, training = training)\n",
    "        res = self.norm2(step_1 + step_3)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim_model, num_heads, dim_feedforward, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention_1 = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = dim_model//num_heads)\n",
    "        self.multi_head_attention_2 = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = dim_model//num_heads)\n",
    "        self.feedforward = feed_forward(dim_model, dim_feedforward)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, encode_res, training, look_ahead_mask = None, padding_mask = None):\n",
    "        attention = self.multi_head_attention_1(query = x, value = x, key = x, attention_mask = look_ahead_mask)\n",
    "        attention = self.dropout1(attention, training = training)\n",
    "        step_1 = self.norm1(x + attention)\n",
    "\n",
    "        attention2 = self.multi_head_attention_2(query = step_1, value =encode_res, key = encode_res, attention_mask = padding_mask)\n",
    "        attention2 = self.dropout2(attention2, training = training)\n",
    "        step_2 = self.norm2(step_1 + attention2)\n",
    "\n",
    "        step_3 = self.feedforward(step_2)\n",
    "        step_4 = self.dropout3(step_3, training = training)\n",
    "        res = self.norm3(step_2 + step_4)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder, Decoder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, dim_model, num_heads, dim_feedforward, input_size, max_pos_encoding, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_size, dim_model)\n",
    "        self.pos_encoding = positional_encoding(max_pos_encoding, dim_model)\n",
    "        self.encode_layers = [EncoderLayer(dim_model, num_heads, dim_feedforward, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate) # don't know if this is needed tho\n",
    "\n",
    "    def call(self, x, training, mask = None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding * tf.math.sqrt(tf.cast(self.dim_model, tf.float32)) + self.pos_encoding[:seq_len, :]\n",
    "        x = self.dropout(x, training = training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encode_layers[i](x, training, mask)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, dim_model, num_heads, dim_feedforward, input_size, max_pos_encoding, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_size, dim_model)\n",
    "        self.pos_encoding = positional_encoding(max_pos_encoding, dim_model)\n",
    "        self.decode_layers = [DecoderLayer(dim_model, num_heads, dim_feedforward, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate) # don't know if this is needed tho, hopefully yes\n",
    "\n",
    "    def call(self, x, encode_output, training, look_ahead_mask = None, padding_mask = None ):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding * tf.math.sqrt(tf.cast(self.dim_model, tf.float32)) + self.pos_encoding[:seq_len, :]\n",
    "        x = self.dropout(x, training = training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decode_layers[i](x, encode_output, training, look_ahead_mask, padding_mask)\n",
    "        return x\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, dim_model, num_heads, dim_feedforward,\n",
    "                 input_size, target_size,\n",
    "                 pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.look_ahead_mask = create_look_ahead_mask(pe_target)\n",
    "        self.encoder = Encoder(num_layers, dim_model, num_heads, dim_feedforward,\n",
    "                               input_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, dim_model, num_heads, dim_feedforward,\n",
    "                               target_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_size)\n",
    "    \n",
    "    def call(self, input, target, training, look_ahead_mask = None):\n",
    "        if look_ahead_mask == None:\n",
    "            look_ahead_mask = self.look_ahead_mask\n",
    "        encode_output = self.encoder(input, training, mask = None)\n",
    "        decode_output = self.decoder(target, encode_output, training, look_ahead_mask, padding_mask = None)\n",
    "        final_output = self.final_layer(decode_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "num_layers = 1\n",
    "dim_model = 128\n",
    "dim_feedforward = 512\n",
    "num_heads = 8\n",
    "input_size = 10101\n",
    "target_size = 8000\n",
    "max_seq_len = 50\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_correct = Transformer(\n",
    "    num_layers = num_layers,\n",
    "    dim_model = dim_model,\n",
    "    num_heads = num_heads,\n",
    "    dim_feedforward = dim_feedforward,\n",
    "    input_size = input_size,\n",
    "    target_size = target_size,\n",
    "    pe_input = max_seq_len,\n",
    "    pe_target = max_seq_len,\n",
    "    dropout_rate = dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    alpha=0.0\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(tf.math.not_equal(real, 0), dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "auto_correct.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "num_samples = 10000\n",
    "sequence_length = 128\n",
    "vocab_size = 20000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df_train[\"sentence\"], df_train[\"processed\"]))\n",
    "dataset = dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='transformer_checkpoint.h5',\n",
    "                                       monitor='loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "EPOCHS = 50\n",
    "history = auto_correct.fit(dataset, epochs=EPOCHS, verbose = 2, callbacks=callbacks)\n",
    "\n",
    "print(\"Training loss history:\", history.history['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
