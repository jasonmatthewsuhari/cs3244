{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.functional import sigmoid\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, rgb_images, edge_images, labels, transform=None):\n",
    "        self.rgb_images = rgb_images  # (num, 224, 224, 3)\n",
    "        self.edge_images = np.expand_dims(edge_images, axis=-1)  # Ensure (num, 224, 224, 1)\n",
    "        self.labels = labels  # (num, 11)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to PyTorch tensors\n",
    "        rgb = torch.tensor(self.rgb_images[idx], dtype=torch.float32)\n",
    "        edge = torch.tensor(self.edge_images[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        # Permute from (H, W, C) â†’ (C, H, W)\n",
    "        rgb = rgb.permute(2, 0, 1)  # (3, 224, 224)\n",
    "        edge = edge.permute(2, 0, 1)  # (1, 224, 224)\n",
    "        # Concatenate along channel dimension\n",
    "        combined = torch.cat((rgb, edge), dim=0)  # (4, 224, 224)\n",
    "        # Normalize image pixels to [0,1]\n",
    "        combined /= 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            combined = self.transform(combined)\n",
    "        \n",
    "        return combined, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109176, 224, 224, 3) uint8 <class 'numpy.ndarray'> (224, 224, 3)\n",
      "(109176, 224, 224) uint8 <class 'numpy.ndarray'> (224, 224)\n",
      "(109176, 11) int64 <class 'numpy.ndarray'> [0 0 0 0 1 0 0 0 0 0 0] 1\n"
     ]
    }
   ],
   "source": [
    "# Load images and labels\n",
    "rgb_train= np.load(\"img_train_plaintext_top11_rgb.npy\")  # Shape: (num_train, 224, 224, 3)\n",
    "edges_train = np.load(\"img_train_plaintext_top11_edges.npy\")  # Shape: (num_train, 224, 224)\n",
    "labels_train = np.load(\"img_train_plaintext_top11_labels.npy\")  # Shape: (num_train, 11) - Multi-label one-hot\n",
    "\n",
    "rgb_valid= np.load(\"img_valid_plaintext_top11_rgb.npy\")  # Shape: (num_valid, 224, 224, 3)\n",
    "edges_valid = np.load(\"img_valid_plaintext_top11_edges.npy\")  # Shape: (num_valid, 224, 224)\n",
    "labels_valid = np.load(\"img_valid_plaintext_top11_labels.npy\")  # Shape: (num_valid, 11)\n",
    "\n",
    "rgb_test = np.load(\"img_test_plaintext_top11_rgb.npy\")  # Shape: (num_valid, 224, 224, 3)\n",
    "edges_test = np.load(\"img_test_plaintext_top11_edges.npy\")  # Shape: (num_valid, 224, 224)\n",
    "labels_test = np.load(\"img_test_plaintext_top11_labels.npy\")  # Shape: (num_valid, 11)\n",
    "\n",
    "print(rgb_train.shape, rgb_train.dtype, type(rgb_train[0]), rgb_train[0].shape)\n",
    "print(edges_train.shape, edges_train.dtype, type(edges_train[0]), edges_train[0].shape)\n",
    "print(labels_train.shape, labels_train.dtype, type(labels_train[0]), labels_train[0], labels_train[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (optional)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "    transforms.RandomRotation(10),\n",
    "])\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = EmojiDataset(rgb_train, edges_train, labels_train, transform=transform)\n",
    "valid_dataset = EmojiDataset(rgb_valid, edges_valid, labels_valid, transform=None)\n",
    "test_dataset = EmojiDataset(rgb_test, edges_test, labels_test, transform=None)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image shape: torch.Size([4, 224, 224])\n",
      "Single label shape: torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "combined, label = train_dataset[0]\n",
    "\n",
    "print(\"Single image shape:\", combined.shape)  # Expected: (4, 224, 224)\n",
    "print(\"Single label shape:\", label.shape)  # Expected: (11,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch image shape: torch.Size([32, 4, 224, 224])\n",
      "Batch label shape: torch.Size([32, 11])\n"
     ]
    }
   ],
   "source": [
    "for combined, labels in train_loader:\n",
    "    print(\"Batch image shape:\", combined.shape)  # Expected: (32, 4, 224, 224)\n",
    "    print(\"Batch label shape:\", labels.shape)  # Expected: (32, 11)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(4, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=11, bias=True)\n",
      "  )\n",
      ")\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained ResNet18\n",
    "model = models.vit_b_16(weights='DEFAULT') \n",
    "\n",
    "num_classes = 11  # Top 11\n",
    "\n",
    "# Modify for 4-channel (RGB + Edge)\n",
    "model.conv_proj = nn.Conv2d(4, 768, kernel_size=16, stride=16)\n",
    "# Modify the last layer\n",
    "model.heads[-1] = nn.Linear(model.heads[-1].in_features, num_classes)\n",
    "print(model)\n",
    "\n",
    "# Move model to GPU\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.2799856848359667 , Valid Loss: 0.27959847943019617\n",
      "Epoch 2, Train Loss: 0.27829388131203014 , Valid Loss: 0.27830664139418376\n",
      "Epoch 3, Train Loss: 0.2780966746237605 , Valid Loss: 0.27992742437691914\n",
      "Epoch 4, Train Loss: 0.2782165509193332 , Valid Loss: 0.2775026378098619\n",
      "Epoch 5, Train Loss: 0.27678251730721415 , Valid Loss: 0.2772192495367514\n",
      "Epoch 6, Train Loss: 0.27661163907974157 , Valid Loss: 0.2774948986670958\n",
      "Epoch 7, Train Loss: 0.27645730166090055 , Valid Loss: 0.27675641146759505\n",
      "Epoch 8, Train Loss: 0.2761825458010154 , Valid Loss: 0.2768122623600657\n",
      "Epoch 9, Train Loss: 0.2759941832052166 , Valid Loss: 0.277450978834793\n",
      "Epoch 10, Train Loss: 0.2759568462364489 , Valid Loss: 0.2765492434618334\n",
      "Epoch 11, Train Loss: 0.27593947463130614 , Valid Loss: 0.2768029755070096\n",
      "Epoch 12, Train Loss: 0.2758203752356355 , Valid Loss: 0.2763072745431037\n",
      "Epoch 13, Train Loss: 0.2761601394813198 , Valid Loss: 0.2763211045157972\n",
      "Epoch 14, Train Loss: 0.2757256932923481 , Valid Loss: 0.27620660915734274\n",
      "Epoch 15, Train Loss: 0.2755796158815115 , Valid Loss: 0.2762458775015104\n",
      "Epoch 16, Train Loss: 0.2754123882594729 , Valid Loss: 0.27595173685796676\n",
      "Epoch 17, Train Loss: 0.2756366990550322 , Valid Loss: 0.2762516897033762\n",
      "Epoch 18, Train Loss: 0.2754372805793707 , Valid Loss: 0.27617459657488674\n",
      "Epoch 19, Train Loss: 0.27555466986153215 , Valid Loss: 0.2759673247340495\n",
      "Epoch 20, Train Loss: 0.2753609009480141 , Valid Loss: 0.2758737449686994\n"
     ]
    }
   ],
   "source": [
    "# Define loss function & optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Multi-label loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 20  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # After each epoch, validate on the validation set\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader)} , Valid Loss: {valid_loss/len(valid_loader)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.79%\n",
      "Test Precision: 9.07%\n",
      "Test Recall: 5.45%\n",
      "Test F1: 6.62% \n",
      "\n",
      "Threshold: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.17%\n",
      "Test Precision: 0.00%\n",
      "Test Recall: 0.00%\n",
      "Test F1: 0.00% \n",
      "\n",
      "Threshold: 0.7\n",
      "Test Accuracy: 90.17%\n",
      "Test Precision: 0.00%\n",
      "Test Recall: 0.00%\n",
      "Test F1: 0.00% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for threshold in (0.3, 0.5, 0.7):\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            # Convert logits to probabilities\n",
    "            probs = sigmoid(outputs)  \n",
    "            # Convert logits to binary, \n",
    "            # Threshold at 0.5 (adjustable) to get binary labels\n",
    "            predicted = (probs > threshold).float()\n",
    "            \n",
    "            # Store predictions and actual labels\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.numel()  # Count total number of elements\n",
    "\n",
    "    # Convert list of numpy arrays into a single numpy array\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    # Compute confusion matrix, precision, and recall for each label (emoji)\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = 100 * precision_score(all_labels, all_preds, average=\"macro\")  # Use 'macro' for multi-label\n",
    "    recall = 100 * recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    f1 = 100 * f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Test Precision: {precision:.2f}%\")\n",
    "    print(f\"Test Recall: {recall:.2f}%\")\n",
    "    print(f\"Test F1: {f1:.2f}% \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
