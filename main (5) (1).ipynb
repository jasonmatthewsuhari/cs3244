{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Twemoji Classifier ‚Äì CS3244 AY24/25 Sem 2\n",
    "\n",
    "**Group Members:**  \n",
    "- Jason Matthew Suhari  \n",
    "- Bryan Castorius Halim  \n",
    "- Nigel Eng Wee Kiat  \n",
    "- Muhammad Salman Al Farisi  \n",
    "- Ng Jia Hao Sherwin  \n",
    "- Ryan Justyn\n",
    "\n",
    "This notebook builds and evaluates baseline models for classifying tweets into emojis using the Twemoji dataset. It's the main entry point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup üì©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Utility Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 ML Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    top_k_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Caching Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model as keras_load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 (Optional) Loading .env file + AWS setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "s3 = session.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. S3 Bucket Data Loading ü™£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'urls.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# all of the s3 item urls are kept in urls.txt, maybe we should wrap this into a function in case it isnt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murls.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     urls \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines() \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m      6\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'urls.txt'"
     ]
    }
   ],
   "source": [
    "# all of the s3 item urls are kept in urls.txt, maybe we should wrap this into a function in case it isnt\n",
    "with open(\"urls.txt\", \"r\") as f:\n",
    "    urls = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "\n",
    "loaded_data = {}\n",
    "for url in urls:\n",
    "    filename = os.path.basename(url)\n",
    "    filepath = os.path.join(\"data\", filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename} from the s3 bucket...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {filename}: {response.status_code}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "\n",
    "    try:\n",
    "        if filename.endswith(\".npy\"):\n",
    "            loaded_data[filename] = np.load(filepath, allow_pickle=True)\n",
    "        elif filename.endswith(\".csv\"):\n",
    "            loaded_data[filename] = pd.read_csv(filepath)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis (EDA) üîç\n",
    "Full EDA code can be found in the eda.ipynb folder. To save space in the main notebook, we have collated just the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "plot_folder = \"plots/\"\n",
    "image_files = sorted([\n",
    "    f for f in os.listdir(plot_folder)\n",
    "    if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "])\n",
    "\n",
    "html_str = \"\"\"\n",
    "<style>\n",
    "  body {\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "  }\n",
    "\n",
    "  .grid-wrapper {\n",
    "    padding: 40px 20px 60px 20px;\n",
    "  }\n",
    "\n",
    "  .grid-container {\n",
    "    display: grid;\n",
    "    grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));\n",
    "    gap: 16px;\n",
    "  }\n",
    "\n",
    "  .grid-item {\n",
    "    text-align: center;\n",
    "  }\n",
    "\n",
    "  .grid-item img {\n",
    "    width: 100%;\n",
    "    height: auto;\n",
    "    border-radius: 8px;\n",
    "    cursor: pointer;\n",
    "    box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
    "    transition: transform 0.2s ease-in-out;\n",
    "  }\n",
    "\n",
    "  .grid-item img:hover {\n",
    "    transform: scale(1.05);\n",
    "  }\n",
    "\n",
    "  dialog::backdrop {\n",
    "    background: rgba(0, 0, 0, 0.9);\n",
    "  }\n",
    "\n",
    "  dialog {\n",
    "    border: none;\n",
    "    background: transparent;\n",
    "    padding: 0;\n",
    "    margin: auto;\n",
    "    z-index: 9999;\n",
    "  }\n",
    "\n",
    "  dialog img {\n",
    "    max-width: 90vw;\n",
    "    max-height: 90vh;\n",
    "    display: block;\n",
    "    margin: auto;\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 6px 20px rgba(0,0,0,0.4);\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<div class=\"grid-wrapper\">\n",
    "  <div class=\"grid-container\">\n",
    "\"\"\"\n",
    "\n",
    "for idx, image in enumerate(image_files):\n",
    "    img_path = os.path.join(plot_folder, image).replace(\"\\\\\", \"/\")\n",
    "    html_str += f\"\"\"\n",
    "    <div class=\"grid-item\">\n",
    "      <img src=\"{img_path}\" onclick=\"document.getElementById('dialog{idx}').showModal()\">\n",
    "      <p style=\"font-size: 14px;\">{image}</p>\n",
    "    </div>\n",
    "    <dialog id=\"dialog{idx}\" onclick=\"this.close()\">\n",
    "      <img src=\"{img_path}\" alt=\"{image}\">\n",
    "    </dialog>\n",
    "    \"\"\"\n",
    "\n",
    "html_str += \"\"\"\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "  // Optional: ESC to close the dialog\n",
    "  document.addEventListener(\"keydown\", function(event) {\n",
    "    if (event.key === \"Escape\") {\n",
    "      document.querySelectorAll(\"dialog[open]\").forEach(d => d.close());\n",
    "    }\n",
    "  });\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_str))\n",
    "\n",
    "#TODO: fix the weird clipping that happens when u open up one of the images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocessing ‚öôÔ∏è\n",
    "\n",
    "todo: someone pls add the preproc code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Baseline Model Training üèãÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any of the model training, probably best to explcitly define the data based on train-test-valid, instead of relying on just loaded_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = loaded_data['train_with_bert_embeddings.csv']\n",
    "valid_df = loaded_data['valid_with_bert_embeddings.csv']\n",
    "test_df  = loaded_data['test_with_bert_embeddings.csv']\n",
    "\n",
    "X_train_full = loaded_data['train_bert_embeddings.npy']\n",
    "X_valid_full = loaded_data['valid_bert_embeddings.npy']\n",
    "X_test_full  = loaded_data['test_bert_embeddings.npy']\n",
    "\n",
    "X_train = np.array([X_train_full[i] for i in train_df['embedding_index']])\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "X_valid = np.array([X_valid_full[i] for i in valid_df['embedding_index']])\n",
    "y_valid = valid_df['label'].values\n",
    "\n",
    "X_test = np.array([X_test_full[i] for i in test_df['embedding_index']])\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# for models tht need scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#L2 normalisation - for models that need normalisation\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer(norm='l2')\n",
    "X_train_normalised = normalizer.fit_transform(X_train)\n",
    "X_valid_normalised = normalizer.fit_transform(X_valid)\n",
    "X_test_normalised = normalizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code for random forest model here\n",
    "rf_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_model = LinearSVC(C=1, max_iter=1000, tol=0.0001)\n",
    "svm_model.fit(X_train_normalised, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code for nn model here\n",
    "snn_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4 Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code for cnn model here\n",
    "cnn_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.5 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "logistic_regression_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.6 Image-Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code for image-based model here\n",
    "ib_classifier_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Alternative Models ü§∑‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Caching üåö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Naming the Models / Caching Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = [rf_model, svm_model, snn_model, cnn_model, logistic_regression_model,ib_classifier_model]\n",
    "\n",
    "# make sure this list is updated if you add on to the list of models above\n",
    "model_names = [\n",
    "    \"random_forest\",\n",
    "    \"svm\",\n",
    "    \"simple_neural\",\n",
    "    \"cnn\",\n",
    "    \"logistic_regression\",\n",
    "    \"image\"\n",
    "]\n",
    "\n",
    "assert len(model_names) == len(list_of_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Actually Caching the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.1. Local Cache\n",
    "Would recommend running this so that your testing later on isn't slow / disrupted, but also do run 7.2.2. so that other people will be able to use your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(list_of_models, model_names, save_dir=\"models\"):\n",
    "    for model, name in zip(list_of_models, model_names):\n",
    "        if model is None:\n",
    "            print(f\"Skipping {name} (model is not set up yet aka None)\")\n",
    "            continue\n",
    "\n",
    "        # Keras models\n",
    "        if isinstance(model, Model):\n",
    "            path = os.path.join(save_dir, f\"{name}.keras\")\n",
    "            model.save(path)\n",
    "            print(f\"Saved model: {name}.keras\")\n",
    "\n",
    "        # SKlearn models\n",
    "        else:\n",
    "            path = os.path.join(save_dir, f\"{name}.pkl\")\n",
    "            with open(path, \"wb\") as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"Saved model: {name}.pkl\")\n",
    "\n",
    "save_models(list_of_models, model_names, save_dir=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2. S3 Cache\n",
    "Please use this üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_s3(session, local_dir=\"models\", bucket_name=\"cs3244-twemoji-ay2425-s2\", s3_prefix=\"models/\"):\n",
    "    s3 = session.client(\"s3\")\n",
    "\n",
    "    for filename in os.listdir(local_dir):\n",
    "        local_path = os.path.join(local_dir, filename)\n",
    "        s3_key = os.path.join(s3_prefix, filename)\n",
    "\n",
    "        # check if file alr exists!\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_key)\n",
    "        exists = \"Contents\" in response and any(obj[\"Key\"] == s3_key for obj in response[\"Contents\"])\n",
    "\n",
    "        if exists:\n",
    "            print(f\"Skipping {filename} (already exists in S3)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Uploading {filename} ‚Üí s3://{bucket_name}/{s3_key}\")\n",
    "        s3.upload_file(local_path, bucket_name, s3_key)\n",
    "\n",
    "    print(\"Model saving complete\")\n",
    "\n",
    "save_models_s3(\n",
    "    session=session,\n",
    "    local_dir=\"models\",\n",
    "    bucket_name=\"cs3244-twemoji-ay2425-s2\",\n",
    "    s3_prefix=\"models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Reloading Models üí°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1. Local Loading (Optional / Not Recommended)\n",
    "Not recommended for first-time, you are not likely to have all of the models cached into your models folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_names, model_dir=\"models\"):\n",
    "    loaded_models = {}\n",
    "\n",
    "    for name in model_names:\n",
    "        pkl_path = os.path.join(model_dir, f\"{name}.pkl\")\n",
    "        keras_path = os.path.join(model_dir, f\"{name}.keras\")\n",
    "\n",
    "        if os.path.exists(pkl_path):\n",
    "            with open(pkl_path, \"rb\") as f:\n",
    "                loaded_models[name] = pickle.load(f)\n",
    "            print(f\"Loaded sklearn model: {name}\")\n",
    "        \n",
    "        elif os.path.exists(keras_path):\n",
    "            loaded_models[name] = keras_load_model(keras_path)\n",
    "            print(f\"Loaded Keras model: {name}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Model file for {name} not found.\")\n",
    "\n",
    "    return loaded_models\n",
    "\n",
    "models = load_models(model_names, model_dir=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 S3-Based Loading (Recommended)\n",
    "OK to run regardless of whether you're first-time or not, but do note that this is definitely slower lol so use the local loading option once you're finished loading the cache files into your local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_from_s3(session, model_names, model_dir=\"models\", bucket_name=\"cs3244-twemoji-ay2425-s2\", s3_prefix=\"models/\"):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    s3 = session.client(\"s3\")\n",
    "    loaded_models = {}\n",
    "\n",
    "    for name in model_names:\n",
    "        pkl_path = os.path.join(model_dir, f\"{name}.pkl\")\n",
    "        keras_path = os.path.join(model_dir, f\"{name}.keras\")\n",
    "\n",
    "        # check if we alr loaded itin aka the person running this is\n",
    "        # the one who made and cached this one specific model\n",
    "        if not os.path.exists(pkl_path) and not os.path.exists(keras_path):\n",
    "            print(f\"{name} not found locally. Downloading from S3!\")\n",
    "\n",
    "            for ext in [\"pkl\", \"keras\"]: # if for some reason ur cache is in diff extension, pls extend this list\n",
    "                key = f\"{s3_prefix}{name}.{ext}\"\n",
    "                local_path = os.path.join(model_dir, f\"{name}.{ext}\")\n",
    "\n",
    "                try:\n",
    "                    s3.download_file(bucket_name, key, local_path)\n",
    "                    print(f\"Downloaded {name}.{ext} from S3\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    continue # must be the next extension then \n",
    "\n",
    "        # Now try loading\n",
    "        if os.path.exists(pkl_path):\n",
    "            with open(pkl_path, \"rb\") as f:\n",
    "                loaded_models[name] = pickle.load(f)\n",
    "            print(f\"Loaded sKlearn model: {name}\")\n",
    "\n",
    "        elif os.path.exists(keras_path):\n",
    "            loaded_models[name] = keras_load_model(keras_path)\n",
    "            print(f\"Loaded Keras model: {name}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Model file for {name} not found locally or on S3.\")\n",
    "\n",
    "    return loaded_models\n",
    "\n",
    "models = load_models_from_s3(\n",
    "    session=session,\n",
    "    model_names=model_names,\n",
    "    model_dir=\"models\",  # local save path\n",
    "    bucket_name=\"cs3244-twemoji-ay2425-s2\",\n",
    "    s3_prefix=\"models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Model Evaluations and Comparisons ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(\n",
    "    models,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    scaled_model_names=None,\n",
    "    normalised_model_names = None\n",
    "    show_confusion=False,\n",
    "    k=3\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nCurrently evaluating the model: {name}\")\n",
    "        if scaled_model_names and name in scaled_model_names:\n",
    "            x_val = X_valid_scaled\n",
    "            x_test = X_test_scaled\n",
    "        if normalised_model_names and name in normalised_model_names:\n",
    "            x_val = X_valid_normalised\n",
    "            x_test = X_test_normalised\n",
    "        else:\n",
    "            x_val = X_valid\n",
    "            x_test = X_test\n",
    "\n",
    "        sample = x_test[0].reshape(1, -1)\n",
    "        start_time = time.time()\n",
    "        _ = model.predict(sample)\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_val_prob = model.predict_proba(x_val)\n",
    "            y_test_prob = model.predict_proba(x_test)\n",
    "            y_val_pred = np.argmax(y_val_prob, axis=1)\n",
    "            y_test_pred = np.argmax(y_test_prob, axis=1)\n",
    "            topk_val = top_k_accuracy_score(y_valid, y_val_prob, k=k)\n",
    "            topk_test = top_k_accuracy_score(y_test, y_test_prob, k=k)\n",
    "        else:\n",
    "            y_val_pred = model.predict(x_val)\n",
    "            y_test_pred = model.predict(x_test)\n",
    "            if hasattr(model, \"predict\") and len(y_test_pred.shape) > 1:\n",
    "                y_val_pred = y_val_pred.argmax(axis=1)\n",
    "                y_test_pred = y_test_pred.argmax(axis=1)\n",
    "\n",
    "            topk_val = topk_test = None\n",
    "\n",
    "        acc_val = accuracy_score(y_valid, y_val_pred)\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        macro_f1 = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "\n",
    "        if show_confusion:\n",
    "            cm = confusion_matrix(y_test, y_test_pred)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "            disp.plot(cmap=\"Blues\")\n",
    "            plt.title(f\"Confusion Matrix: {name}\")\n",
    "            plt.show()\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Val Accuracy\": round(acc_val, 4),\n",
    "            \"Test Accuracy\": round(acc_test, 4),\n",
    "            f\"Top-{k} Accuracy\": round(topk_test, 4) if topk_test is not None else \"N/A\",\n",
    "            \"Macro-F1\": round(macro_f1, 4),\n",
    "            \"Latency (ms)\": round(latency_ms, 2)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results).sort_values(\"Test Accuracy\", ascending=False)\n",
    "\n",
    "    # cute little ML podium thing lol for funs sake\n",
    "    podium = df.head(3)\n",
    "    medals = [\"ü•á\", \"ü•à\", \"ü•â\"]\n",
    "    for i in range(len(podium)):\n",
    "        model_name = podium.iloc[i][\"Model\"]\n",
    "        display(Markdown(f\"### {medals[i]} **{model_name}** with {podium.iloc[i]['Test Accuracy']*100:.2f}% test accuracy!\"))\n",
    "    display(df.reset_index(drop=True))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = evaluate_models(\n",
    "    models,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    scaled_model_names=[\"logistic_regression\", \"svm\", \"simple_neural\", \"cnn\"],\n",
    "    show_confusion=True,\n",
    "    k=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Conclusion üòé"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
